import torch
import yaml
import numpy as np
import matplotlib.pyplot as plt
from rl_games.common import env_configurations, vecenv
from rl_games.torch_runner import Runner
from src.envs.env import RLGamesEnv
from src.envs.RLGamesEnvWrapper import RLGamesEnvWrapper
import os

# -------------------------------
# 1ï¸âƒ£ ç¯å¢ƒæ„é€ å‡½æ•°ï¼ˆä¸è®­ç»ƒè„šæœ¬ç›¸åŒï¼‰
# -------------------------------
def create_env(config_name=None, num_envs=1, **kwargs):
    env = RLGamesEnv(
        config_name="env_config.yaml",
        traj_name="traj_config.yaml", 
        num_envs=num_envs,
        device="cuda"
    )
    return RLGamesEnvWrapper(env)

# -------------------------------
# 2ï¸âƒ£ æ³¨å†Œ vecenvï¼ˆä¸è®­ç»ƒè„šæœ¬ç›¸åŒï¼‰
# -------------------------------
vecenv.register(
    "RLGPUEnv",
    lambda config_name, num_actors, **kwargs: create_env(num_envs=num_actors, **kwargs)
)

# -------------------------------
# 3ï¸âƒ£ æ³¨å†Œç¯å¢ƒåˆ° rl-games é…ç½®ï¼ˆä¸è®­ç»ƒè„šæœ¬ç›¸åŒï¼‰
# -------------------------------
env_configurations.register(
    "rlgpu",
    {
        "vecenv_type": "RLGPUEnv", 
        "env_creator": lambda **kwargs: create_env(**kwargs)
    }
)

def create_test_config(base_config_path, checkpoint_path):
    """åŸºäºè®­ç»ƒé…ç½®åˆ›å»ºæµ‹è¯•é…ç½®"""
    with open(base_config_path, "r") as f:
        config = yaml.safe_load(f)
    
    # ä¿®æ”¹ä¸ºæµ‹è¯•æ¨¡å¼
    config['params']['config']['is_train'] = False
    config['params']['config']['num_actors'] = 1  # æµ‹è¯•æ—¶åªç”¨1ä¸ªç¯å¢ƒ
    config['params']['config']['player'] = True   # æ·»åŠ  player æ¨¡å¼
    config['params']['load_checkpoint'] = True
    config['params']['load_path'] = checkpoint_path
    
    # ç¡®ä¿è·¯å¾„æ ¼å¼æ­£ç¡®
    if not os.path.isabs(checkpoint_path):
        config['params']['load_path'] = os.path.abspath(checkpoint_path)
    
    return config

def test_model(config, num_episodes=10):
    """æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹"""
    
    # åˆ›å»º runner å¹¶åŠ è½½é…ç½®
    runner = Runner()
    runner.load(config)
    runner.reset()
    
    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ - ä¿®æ­£è®¿é—®æ–¹å¼
    if hasattr(runner, 'agent') and hasattr(runner.agent, 'model'):
        runner.agent.model.eval()
    elif hasattr(runner, 'algo') and hasattr(runner.algo, 'model'):
        runner.algo.model.eval()
    
    results = []
    
    print(f"å¼€å§‹æµ‹è¯•ï¼Œå…± {num_episodes} ä¸ªå›åˆ...")
    
    for episode in range(num_episodes):
        # ä¿®æ­£ç¯å¢ƒé‡ç½®æ–¹å¼
        obs = runner.env_reset()  # ä½¿ç”¨ runner çš„é‡ç½®æ–¹æ³•
        episode_reward = 0
        episode_length = 0
        done = False
        
        positions = []  # è®°å½•è´Ÿè½½ä½ç½®è½¨è¿¹
        target_positions = []  # è®°å½•ç›®æ ‡ä½ç½®
        rewards = []  # è®°å½•æ¯æ­¥å¥–åŠ±
        
        while not done and episode_length < 1000:
            with torch.no_grad():
                # ä¿®æ­£åŠ¨ä½œè·å–æ–¹å¼
                if hasattr(runner, 'agent'):
                    action = runner.agent.get_action(obs, is_deterministic=True)
                elif hasattr(runner, 'algo'):
                    action = runner.algo.get_action(obs, is_deterministic=True)
                else:
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨æ¨¡å‹
                    obs_tensor = torch.as_tensor(obs, dtype=torch.float32, device='cuda')
                    if len(obs_tensor.shape) == 1:
                        obs_tensor = obs_tensor.unsqueeze(0)
                    action = runner.algo.model.a2c_network(obs_tensor)['mus'].cpu().numpy()
            
            # æ‰§è¡ŒåŠ¨ä½œ - ä¿®æ­£ç¯å¢ƒæ­¥è¿›æ–¹å¼
            obs, reward, done, info = runner.env_step(action)
            
            # å¤„ç†å¥–åŠ±æ ¼å¼
            if isinstance(reward, (list, np.ndarray)):
                reward_val = reward[0] if len(reward) > 0 else 0
            elif isinstance(reward, torch.Tensor):
                reward_val = reward.item() if reward.numel() == 1 else reward[0].item()
            else:
                reward_val = reward
                
            episode_reward += reward_val
            episode_length += 1
            rewards.append(reward_val)
            
            # å¤„ç† done æ ¼å¼
            if isinstance(done, (list, np.ndarray)):
                done = done[0] if len(done) > 0 else False
            elif isinstance(done, torch.Tensor):
                done = done.item() if done.numel() == 1 else done[0].item()
            
            # è®°å½•ä½ç½®ä¿¡æ¯
            if len(info) > 0 and isinstance(info[0], dict):
                if 'current_position' in info[0]:
                    positions.append(info[0]['current_position'].cpu().numpy())
                if 'target_position' in info[0]:
                    target_positions.append(info[0]['target_position'].cpu().numpy())
        
        final_error = info[0].get('pos_error', 0) if len(info) > 0 and isinstance(info[0], dict) else 0
        
        results.append({
            'episode': episode + 1,
            'reward': episode_reward,
            'length': episode_length,
            'positions': np.array(positions) if positions else np.array([]),
            'targets': np.array(target_positions) if target_positions else np.array([]),
            'rewards': np.array(rewards),
            'final_error': final_error
        })
        
        print(f"å›åˆ {episode+1}: æ€»å¥–åŠ±={episode_reward:.2f}, é•¿åº¦={episode_length}, æœ€ç»ˆè¯¯å·®={final_error:.3f}")
    
    return results

def plot_results(results):
    """å¯è§†åŒ–æµ‹è¯•ç»“æœ"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 10))
    
    # 1. å¥–åŠ±åˆ†å¸ƒ
    rewards = [r['reward'] for r in results]
    axes[0,0].bar(range(1, len(rewards)+1), rewards, color='skyblue', alpha=0.7)
    axes[0,0].set_title('æ¯å›åˆæ€»å¥–åŠ±')
    axes[0,0].set_xlabel('å›åˆ')
    axes[0,0].set_ylabel('æ€»å¥–åŠ±')
    axes[0,0].grid(True, alpha=0.3)
    
    # 2. å›åˆé•¿åº¦
    lengths = [r['length'] for r in results]
    axes[0,1].bar(range(1, len(lengths)+1), lengths, color='lightgreen', alpha=0.7)
    axes[0,1].set_title('æ¯å›åˆé•¿åº¦')
    axes[0,1].set_xlabel('å›åˆ')
    axes[0,1].set_ylabel('æ­¥æ•°')
    axes[0,1].grid(True, alpha=0.3)
    
    # 3. æœ€ç»ˆè¯¯å·®
    errors = [r['final_error'] for r in results]
    axes[0,2].bar(range(1, len(errors)+1), errors, color='salmon', alpha=0.7)
    axes[0,2].set_title('æœ€ç»ˆä½ç½®è¯¯å·®')
    axes[0,2].set_xlabel('å›åˆ')
    axes[0,2].set_ylabel('è¯¯å·® (m)')
    axes[0,2].grid(True, alpha=0.3)
    
    # 4. è½¨è¿¹è·Ÿè¸ª (é€‰æ‹©ç¬¬ä¸€ä¸ªå›åˆ)
    if len(results) > 0 and len(results[0]['positions']) > 0:
        pos = results[0]['positions']
        targets = results[0]['targets']
        
        axes[1,0].plot(pos[:, 0], pos[:, 1], 'b-', label='å®é™…è½¨è¿¹', linewidth=2, alpha=0.8)
        axes[1,0].plot(targets[:, 0], targets[:, 1], 'r--', label='ç›®æ ‡è½¨è¿¹', linewidth=2, alpha=0.8)
        axes[1,0].set_title('è½¨è¿¹è·Ÿè¸ªæ•ˆæœ (å›åˆ 1)')
        axes[1,0].set_xlabel('X (m)')
        axes[1,0].set_ylabel('Y (m)')
        axes[1,0].legend()
        axes[1,0].grid(True, alpha=0.3)
        axes[1,0].axis('equal')
    
    # 5. å¥–åŠ±éšæ—¶é—´å˜åŒ– (é€‰æ‹©ç¬¬ä¸€ä¸ªå›åˆ)
    if len(results) > 0 and len(results[0]['rewards']) > 0:
        rewards_over_time = results[0]['rewards']
        axes[1,1].plot(rewards_over_time, 'g-', alpha=0.7)
        axes[1,1].set_title('æ¯æ­¥å¥–åŠ±å˜åŒ– (å›åˆ 1)')
        axes[1,1].set_xlabel('æ­¥æ•°')
        axes[1,1].set_ylabel('å¥–åŠ±')
        axes[1,1].grid(True, alpha=0.3)
    
    # 6. ç»Ÿè®¡åˆ†å¸ƒ
    axes[1,2].hist(rewards, bins=min(10, len(rewards)), alpha=0.7, color='purple')
    axes[1,2].set_title('å¥–åŠ±åˆ†å¸ƒ')
    axes[1,2].set_xlabel('æ€»å¥–åŠ±')
    axes[1,2].set_ylabel('é¢‘æ¬¡')
    axes[1,2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('test_results.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*50)
    print("ğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡")
    print("="*50)
    print(f"ğŸ¯ å¹³å‡å¥–åŠ±: {np.mean(rewards):.2f} Â± {np.std(rewards):.2f}")
    print(f"â±ï¸  å¹³å‡é•¿åº¦: {np.mean(lengths):.1f} Â± {np.std(lengths):.1f}")
    print(f"ğŸ“ å¹³å‡æœ€ç»ˆè¯¯å·®: {np.mean(errors):.3f} Â± {np.std(errors):.3f}")
    print(f"âœ… æˆåŠŸç‡ (è¯¯å·® < 0.1m): {np.mean(np.array(errors) < 0.1)*100:.1f}%")
    print(f"ğŸ† æœ€ä½³å¥–åŠ±: {np.max(rewards):.2f}")
    print(f"ğŸ’¯ æœ€å°è¯¯å·®: {np.min(errors):.3f}")

def main():
    # é…ç½®æ–‡ä»¶è·¯å¾„
    base_config_path = "src/config/ppo_config.yaml"
    
    # æ£€æŸ¥æœ€æ–°çš„æ£€æŸ¥ç‚¹æ–‡ä»¶
    checkpoint_dir = "runs"
    checkpoint_files = []
    
    for root, dirs, files in os.walk(checkpoint_dir):
        for file in files:
            if file.startswith("last_lift_") and file.endswith(".pth"):
                checkpoint_files.append(os.path.join(root, file))
    
    if not checkpoint_files:
        print("âŒ æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ã€‚")
        return
    
    # é€‰æ‹©æœ€æ–°çš„æ£€æŸ¥ç‚¹
    latest_checkpoint = max(checkpoint_files, key=os.path.getmtime)
    print(f"ğŸ”„ ä½¿ç”¨æ£€æŸ¥ç‚¹: {latest_checkpoint}")
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    test_config = create_test_config(base_config_path, latest_checkpoint)
    
    # è¿è¡Œæµ‹è¯•
    print("ğŸš€ å¼€å§‹æ¨¡å‹æµ‹è¯•...")
    results = test_model(test_config, num_episodes=5)
    
    # å¯è§†åŒ–ç»“æœ
    print("\nğŸ“ˆ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
    plot_results(results)
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜ä¸º test_results.png")

if __name__ == "__main__":
    main()